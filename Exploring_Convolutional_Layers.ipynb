{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e9afa05",
   "metadata": {},
   "source": [
    "# Notebook - Convolutional Neural Networks on MNIST\n",
    "**Escuela Colombiana de Ingeniería Julio Garavito**  \n",
    "**Student:** Santiago Botero García\n",
    "\n",
    "This assignment explores convolutional layers as architectural components rather than black-box tools.\n",
    "MNIST is a suitable dataset because:\n",
    "\n",
    "- Images are 2D spatial data (28×28 grayscale)\n",
    "- Digits exhibit translation invariance\n",
    "- Local patterns (edges, strokes) are meaningful\n",
    "- Dataset size is large enough for learning but small enough for fast experimentation\n",
    "\n",
    "This makes MNIST ideal for analyzing the inductive bias introduced by convolution.\n",
    "\n",
    "## Step 0: Setup & Imports\n",
    "- Dependency installation\n",
    "- Imports\n",
    "- Notebook configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53fc9397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in .\\.venv\\Lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: torchvision in .\\.venv\\Lib\\site-packages (0.25.0)\n",
      "Requirement already satisfied: pandas in .\\.venv\\Lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: huggingface-hub in .\\.venv\\Lib\\site-packages (1.3.7)\n",
      "Requirement already satisfied: matplotlib in .\\.venv\\Lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: filelock in .\\.venv\\Lib\\site-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in .\\.venv\\Lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in .\\.venv\\Lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in .\\.venv\\Lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in .\\.venv\\Lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in .\\.venv\\Lib\\site-packages (from torch) (2026.1.0)\n",
      "Requirement already satisfied: setuptools in .\\.venv\\Lib\\site-packages (from torch) (80.10.2)\n",
      "Requirement already satisfied: numpy in .\\.venv\\Lib\\site-packages (from torchvision) (2.4.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in .\\.venv\\Lib\\site-packages (from torchvision) (12.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in .\\.venv\\Lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata in .\\.venv\\Lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in .\\.venv\\Lib\\site-packages (from huggingface-hub) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in .\\.venv\\Lib\\site-packages (from huggingface-hub) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in .\\.venv\\Lib\\site-packages (from huggingface-hub) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in .\\.venv\\Lib\\site-packages (from huggingface-hub) (6.0.3)\n",
      "Requirement already satisfied: shellingham in .\\.venv\\Lib\\site-packages (from huggingface-hub) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in .\\.venv\\Lib\\site-packages (from huggingface-hub) (4.67.3)\n",
      "Requirement already satisfied: typer-slim in .\\.venv\\Lib\\site-packages (from huggingface-hub) (0.21.1)\n",
      "Requirement already satisfied: anyio in .\\.venv\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub) (4.12.1)\n",
      "Requirement already satisfied: certifi in .\\.venv\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in .\\.venv\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub) (1.0.9)\n",
      "Requirement already satisfied: idna in .\\.venv\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in .\\.venv\\Lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub) (0.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in .\\.venv\\Lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in .\\.venv\\Lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in .\\.venv\\Lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in .\\.venv\\Lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in .\\.venv\\Lib\\site-packages (from matplotlib) (3.3.2)\n",
      "Requirement already satisfied: six>=1.5 in .\\.venv\\Lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in .\\.venv\\Lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in .\\.venv\\Lib\\site-packages (from tqdm>=4.42.1->huggingface-hub) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in .\\.venv\\Lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: click>=8.0.0 in .\\.venv\\Lib\\site-packages (from typer-slim->huggingface-hub) (8.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision pandas huggingface-hub matplotlib\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85531de8",
   "metadata": {},
   "source": [
    "## Step 1: Load and Prepare the Dataset\n",
    "### Downloading the Dataset\n",
    "We load the MNIST dataset directly from Hugging Face using the datasets library. MNIST is a well-known benchmark dataset consisting of 70,000 grayscale images of handwritten digits, each with a resolution of 28×28 pixels. The dataset is already preprocessed and split into training and test sets, eliminating the need for manual downloading or formatting.\n",
    "\n",
    "Each data sample includes a PIL image and an associated label representing the digit class (0–9). The training split contains 60,000 images, while the test split contains 10,000 images. Images are automatically decoded when accessed, allowing for straightforward integration into image classification pipelines.\n",
    "\n",
    "This setup allows us to focus on model development and evaluation without additional preprocessing overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "040fb122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 31.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\Users\\\\santi\\\\OneDrive\\\\Documents\\\\Convolutional-Layers-Through-Data-and-Experiments'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot_download(\"ylecun/mnist\", repo_type=\"dataset\", local_dir=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a80053d",
   "metadata": {},
   "source": [
    "## Step 2: Implement Convolutional Neural Network\n",
    "\n",
    "### Model and Training Components\n",
    "\n",
    "This section contains the definitions of:\n",
    "\n",
    "* **ConvolutionalBlock**\n",
    "  Responsibility: Local feature extraction.\n",
    "  Implements a single convolutional layer followed by an activation function and max pooling. Encapsulates low-level feature processing used in the CNN.\n",
    "\n",
    "* **CNNClassifier**\n",
    "  Responsibility: Define network topology.\n",
    "  Builds an end-to-end CNN for MNIST classification by stacking `ConvolutionalBlock`s and adding fully connected layers for classification. Handles the forward propagation of input images through the network.\n",
    "\n",
    "* **Trainer**\n",
    "  Responsibility: Optimization loop and metrics.\n",
    "  Manages training and evaluation of the CNN. Implements the training loop, computes loss using cross-entropy, performs backpropagation, updates model parameters using the Adam optimizer, and calculates accuracy on test data.\n",
    "\n",
    "* **build_dataloaders**\n",
    "  Responsibility: Prepare data for training and testing.\n",
    "  Sets up PyTorch DataLoaders for MNIST, including transformations such as normalization and conversion to tensors, and allows batching and shuffling of data.\n",
    "\n",
    "These classes and functions together provide the full pipeline for training, evaluating, and testing a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80edc6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Encapsulates a convolutional layer followed by activation and pooling.\n",
    "    Responsibility: Local feature extraction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int = 3,\n",
    "        activation: nn.Module = nn.ReLU(),\n",
    "        pool_kernel: int = 2,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=kernel_size // 2,\n",
    "        )\n",
    "        self.activation = activation\n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_kernel)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    End-to-end CNN architecture for MNIST classification.\n",
    "    Responsibility: Define network topology.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int = 10) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            ConvolutionalBlock(1, 32),\n",
    "            ConvolutionalBlock(32, 64),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Orchestrates training and evaluation.\n",
    "    Responsibility: Optimization loop and metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        device: torch.device,\n",
    "        lr: float = 1e-3,\n",
    "        weight_decay: float = 0.0,\n",
    "    ) -> None:\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "\n",
    "    def train_epoch(self, loader: DataLoader) -> float:\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, labels in loader:\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        return running_loss / len(loader)\n",
    "\n",
    "    def evaluate(self, loader: DataLoader) -> float:\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                outputs = self.model(images)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        return correct / total\n",
    "\n",
    "def build_dataloaders(\n",
    "    batch_size: int,\n",
    "    num_workers: int = 0,\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_dataset = datasets.MNIST(\n",
    "        root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    test_dataset = datasets.MNIST(\n",
    "        root=\"data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddbd928",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "Load the MNIST dataset into training and test DataLoaders with a batch size of 64:\n",
    "\n",
    "- `train_loader`: Shuffled batches for training.\n",
    "- `test_loader`: Sequential batches for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7dd357c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = build_dataloaders(batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d8606",
   "metadata": {},
   "source": [
    "### Inspecting the Dataset\n",
    "Examine the distribution of labels in the training and test sets to ensure that all classes are balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b3a5f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label distribution:\n",
      " 0    5923\n",
      "1    6742\n",
      "2    5958\n",
      "3    6131\n",
      "4    5842\n",
      "5    5421\n",
      "6    5918\n",
      "7    6265\n",
      "8    5851\n",
      "9    5949\n",
      "Name: count, dtype: int64\n",
      "Test label distribution:\n",
      " 0     980\n",
      "1    1135\n",
      "2    1032\n",
      "3    1010\n",
      "4     982\n",
      "5     892\n",
      "6     958\n",
      "7    1028\n",
      "8     974\n",
      "9    1009\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_loader.dataset\n",
    "labels = pd.Series([label for _, label in train_dataset])\n",
    "label_counts = labels.value_counts().sort_index()\n",
    "print(\"Train label distribution:\\n\", label_counts)\n",
    "\n",
    "test_dataset = test_loader.dataset\n",
    "test_labels = pd.Series([label for _, label in test_dataset])\n",
    "print(\"Test label distribution:\\n\", test_labels.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7f1e2b",
   "metadata": {},
   "source": [
    "### Inspect a single data sample\n",
    "Retrieve and inspect the first image-label pair from the training dataset:\n",
    "\n",
    "- Print the shape of the image tensor.\n",
    "- Print its corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1692fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 28, 28])\n",
      "Label: 5\n"
     ]
    }
   ],
   "source": [
    "image, label = train_dataset[0]\n",
    "print(\"Image shape:\", image.shape)\n",
    "print(\"Label:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc51cde8",
   "metadata": {},
   "source": [
    "### Visualize sample images\n",
    "Display a small grid of 10 sample images from the training dataset with their labels.  \n",
    "\n",
    "- Helps verify that the data has loaded correctly.\n",
    "- Provides a quick visual sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b648ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    img, label = train_dataset[i]\n",
    "    ax.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    ax.set_title(f\"Label: {label}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f958f15",
   "metadata": {},
   "source": [
    "## Step 3: Baseline Model (Non-Convolutional)\n",
    "### Define a baseline MLP model\n",
    "We define a simple fully connected neural network (MLP) for MNIST classification:\n",
    "\n",
    "- Input layer flattens the 28×28 images into vectors.\n",
    "- Hidden layer: 256 neurons with ReLU activation.\n",
    "- Output layer: 10 classes for digit classification.\n",
    "\n",
    "This model serves as a baseline for comparison with the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff82318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBaseline(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e7ae39",
   "metadata": {},
   "source": [
    "### Utility to count trainable parameters\n",
    "Define a helper function `count_parameters` to compute the total number of trainable parameters in a model.\n",
    "\n",
    "- Useful for comparing model complexity and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c9f542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d836ed5",
   "metadata": {},
   "source": [
    "### Train and evaluate the MLP baseline\n",
    "- Set the device to GPU if available, otherwise CPU.\n",
    "- Initialize the MLP baseline model and a `Trainer` instance.\n",
    "- Train the model for 5 epochs and evaluate on the test set.\n",
    "- Print training loss and test accuracy after each epoch.\n",
    "- Display the total number of trainable parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6073416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=0.2250, acc=0.9514\n",
      "Epoch 2: loss=0.0954, acc=0.9737\n",
      "Epoch 3: loss=0.0642, acc=0.9754\n",
      "Epoch 4: loss=0.0504, acc=0.9770\n",
      "Epoch 5: loss=0.0365, acc=0.9780\n",
      "Baseline parameters: 203530\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "baseline_model = MLPBaseline()\n",
    "baseline_trainer = Trainer(baseline_model, device)\n",
    "\n",
    "for epoch in range(5):\n",
    "    loss = baseline_trainer.train_epoch(train_loader)\n",
    "    acc = baseline_trainer.evaluate(test_loader)\n",
    "    print(f\"Epoch {epoch+1}: loss={loss:.4f}, acc={acc:.4f}\")\n",
    "\n",
    "print(\"Baseline parameters:\", count_parameters(baseline_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c19a73",
   "metadata": {},
   "source": [
    "## Step 4: CNN Architecture\n",
    "### Train and evaluate the CNN model\n",
    "- Initialize the CNN classifier and a `Trainer` instance.\n",
    "- Train the CNN for 5 epochs using the training DataLoader.\n",
    "- Evaluate the model on the test set after each epoch, printing loss and accuracy.\n",
    "- Print the total number of trainable parameters in the CNN.\n",
    "\n",
    "This allows us to compare performance and complexity of the CNN versus the MLP baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6e61c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=0.1341, acc=0.9856\n",
      "Epoch 2: loss=0.0414, acc=0.9875\n",
      "Epoch 3: loss=0.0285, acc=0.9865\n",
      "Epoch 4: loss=0.0205, acc=0.9881\n",
      "Epoch 5: loss=0.0159, acc=0.9886\n",
      "CNN parameters: 421642\n"
     ]
    }
   ],
   "source": [
    "cnn_model = CNNClassifier()\n",
    "cnn_trainer = Trainer(cnn_model, device)\n",
    "\n",
    "for epoch in range(5):\n",
    "    loss = cnn_trainer.train_epoch(train_loader)\n",
    "    acc = cnn_trainer.evaluate(test_loader)\n",
    "    print(f\"Epoch {epoch+1}: loss={loss:.4f}, acc={acc:.4f}\")\n",
    "\n",
    "print(\"CNN parameters:\", count_parameters(cnn_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9504731",
   "metadata": {},
   "source": [
    "## Step 5: Controlled Convolution Experiment\n",
    "### Define a CNN with variable kernel size\n",
    "- Create a CNN class (`CNNWithKernel`) that allows specifying the convolutional kernel size.\n",
    "- Uses two convolutional blocks with the given kernel size, followed by fully connected layers.\n",
    "- Forward pass applies feature extraction then classification.\n",
    "\n",
    "This lets us experiment with how kernel size affects model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "860a6b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNWithKernel(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            ConvolutionalBlock(1, 32, kernel_size=kernel_size),\n",
    "            ConvolutionalBlock(32, 64, kernel_size=kernel_size),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e4d1d3",
   "metadata": {},
   "source": [
    "### Experiment with different kernel sizes\n",
    "- Train CNN models with kernel sizes 3×3 and 5×5 for 3 epochs each.\n",
    "- Evaluate test accuracy for each configuration.\n",
    "- Store and print the results for comparison.\n",
    "\n",
    "Purpose: Investigate the impact of convolutional kernel size on classification accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e349d41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel 3x3 -> accuracy: 0.9896\n",
      "Kernel 5x5 -> accuracy: 0.9831\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for k in [3, 5]:\n",
    "    model = CNNWithKernel(kernel_size=k)\n",
    "    trainer = Trainer(model, device)\n",
    "\n",
    "    for _ in range(3):\n",
    "        trainer.train_epoch(train_loader)\n",
    "\n",
    "    acc = trainer.evaluate(test_loader)\n",
    "    results[k] = acc\n",
    "    print(f\"Kernel {k}x{k} -> accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6113bde8",
   "metadata": {},
   "source": [
    "## Step 6: Interpretation & Architectural Reasoning\n",
    "## Interpretation and Architectural Reasoning\n",
    "\n",
    "### Why did convolutional layers outperform the baseline?\n",
    "\n",
    "The convolutional neural network outperformed the fully connected baseline because it explicitly exploits the spatial structure of image data. \n",
    "While the baseline model treats each pixel as an independent feature, convolutional layers operate on local neighborhoods, allowing the model to learn meaningful patterns such as edges, corners, and strokes.\n",
    "\n",
    "Additionally, convolutional layers use weight sharing, which drastically reduces the number of trainable parameters compared to a dense architecture. \n",
    "This not only improves computational efficiency but also acts as a form of regularization, making the model less prone to overfitting. \n",
    "As a result, the CNN is able to generalize better despite having fewer parameters than the baseline model.\n",
    "\n",
    "Pooling layers further improve performance by introducing a degree of translation invariance. \n",
    "Small shifts or distortions in handwritten digits do not significantly affect the learned representations, which is a desirable property for image classification tasks such as MNIST.\n",
    "\n",
    "### What inductive bias does convolution introduce?\n",
    "\n",
    "Convolutional layers introduce a strong inductive bias based on three key assumptions:\n",
    "\n",
    "1. **Locality**: Nearby pixels are more likely to be related than distant ones.  \n",
    "2. **Stationarity**: The same features (e.g., edges or textures) can appear anywhere in the image.  \n",
    "3. **Compositionality**: Complex patterns can be built hierarchically from simpler local features.\n",
    "\n",
    "These assumptions are well aligned with natural images and handwritten digits, where local structures repeat across different spatial locations. \n",
    "By embedding these assumptions directly into the architecture, convolutional networks restrict the hypothesis space to functions that respect spatial coherence, enabling more efficient learning from data.\n",
    "\n",
    "### In what type of problems would convolution not be appropriate?\n",
    "\n",
    "Convolutional architectures are not universally suitable and may perform poorly when their inductive bias does not match the underlying data structure. \n",
    "For example, convolution is generally inappropriate for:\n",
    "\n",
    "- **Tabular data**, where feature order and spatial proximity have no inherent meaning.\n",
    "- **Data with unique positional semantics**, such as structured symbolic inputs where each position encodes a distinct concept.\n",
    "- **Highly relational or sequential data**, where long-range dependencies dominate and spatial locality is weak (e.g., graphs or certain natural language tasks).\n",
    "\n",
    "In such cases, alternative architectures such as fully connected networks, recurrent models, transformers, or graph neural networks may provide a better inductive bias for the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06af56c0",
   "metadata": {},
   "source": [
    "## Step 7: Model Packaging and Deployment Preparation (SageMaker)\n",
    "\n",
    "In this section, the trained convolutional model is prepared for deployment using\n",
    "Amazon SageMaker's PyTorch serving infrastructure.\n",
    "\n",
    "The deployment workflow includes:\n",
    "\n",
    "- Saving the trained model weights\n",
    "- Defining an inference interface compatible with SageMaker\n",
    "- Performing a local inference test\n",
    "- Packaging model artifacts for upload to Amazon S3\n",
    "\n",
    "This process mirrors a real-world production deployment pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "50adfe8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to model\\model.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "MODEL_DIR = \"model\"\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"model.pth\")\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "torch.save(cnn_model.state_dict(), MODEL_PATH)\n",
    "\n",
    "print(f\"Model saved to {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3c789c",
   "metadata": {},
   "source": [
    "### SageMaker Inference Interface\n",
    "\n",
    "SageMaker PyTorch containers expect a set of standardized functions\n",
    "to handle model loading and inference:\n",
    "\n",
    "- `model_fn`: loads the trained model\n",
    "- `input_fn`: processes incoming requests\n",
    "- `predict_fn`: performs inference\n",
    "- `output_fn`: formats the response\n",
    "\n",
    "These functions are defined in a separate `inference.py` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6931f6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/inference.py\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvolutionalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=kernel_size // 2\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pool(self.relu(self.conv(x)))\n",
    "\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            ConvolutionalBlock(1, 32),\n",
    "            ConvolutionalBlock(32, 64),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model = CNNClassifier()\n",
    "    model.load_state_dict(\n",
    "        torch.load(f\"{model_dir}/model.pth\", map_location=\"cpu\")\n",
    "    )\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    if request_content_type == \"application/json\":\n",
    "        data = json.loads(request_body)\n",
    "        tensor = torch.tensor(data[\"inputs\"], dtype=torch.float32)\n",
    "        return tensor\n",
    "    raise ValueError(\"Unsupported content type\")\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_data)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    return json.dumps({\"prediction\": prediction.tolist()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a6d0a2",
   "metadata": {},
   "source": [
    "### Local Inference Test\n",
    "\n",
    "Before deploying the model to SageMaker, a local inference test is performed\n",
    "to verify that the saved model and inference interface work correctly.\n",
    "\n",
    "This simulates the behavior of a SageMaker endpoint using the same inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6d3cba56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True labels:      [7, 2, 1, 0, 4]\n",
      "Predicted labels: [7, 2, 1, 0, 4]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import importlib\n",
    "from model import inference\n",
    "\n",
    "# Load model locally\n",
    "importlib.reload(inference)\n",
    "\n",
    "model = inference.model_fn(\"model\")\n",
    "\n",
    "# Example batch from test set\n",
    "sample_images, sample_labels = next(iter(test_loader))\n",
    "\n",
    "# Prepare fake SageMaker request\n",
    "request = json.dumps({\n",
    "    \"inputs\": sample_images[:5].tolist()\n",
    "})\n",
    "\n",
    "# Run inference\n",
    "input_data = inference.input_fn(request, \"application/json\")\n",
    "predictions = inference.predict_fn(input_data, model)\n",
    "\n",
    "print(\"True labels:     \", sample_labels[:5].tolist())\n",
    "print(\"Predicted labels:\", predictions.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed15ad",
   "metadata": {},
   "source": [
    "### Packaging Model Artifacts\n",
    "\n",
    "To deploy the model to SageMaker, the `model/` directory must be compressed\n",
    "into a `.tar.gz` file and uploaded to an Amazon S3 bucket.\n",
    "\n",
    "This archive contains both the trained model weights and the inference code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "71ae518e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.tar.gz created successfully\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open(\"model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"model\", arcname=\"model\")\n",
    "\n",
    "print(\"model.tar.gz created successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a41c7a4",
   "metadata": {},
   "source": [
    "### SageMaker Deployment Notes\n",
    "\n",
    "The generated `model.tar.gz` file can be uploaded to Amazon S3 and deployed\n",
    "using a PyTorch SageMaker model.\n",
    "\n",
    "Due to IAM permission restrictions, the endpoint could not be created in this environment.\n",
    "However, all required artifacts and interfaces are fully compliant with SageMaker's\n",
    "PyTorch serving requirements.\n",
    "\n",
    "Local inference tests confirm that the model behaves correctly and is ready\n",
    "for deployment once permissions are available.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
